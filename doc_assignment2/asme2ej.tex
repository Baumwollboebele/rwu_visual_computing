%%%%%%%%%%%%%%%%%%%%%%%%%%% asme2ej.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for producing ASME-format journal articles using LaTeX    %
% Written by   Harry H. Cheng, Professor and Director                %
%              Integration Engineering Laboratory                    %
%              Department of Mechanical and Aeronautical Engineering %
%              University of California                              %
%              Davis, CA 95616                                       %
%              Tel: (530) 752-5020 (office)                          %
%                   (530) 752-1028 (lab)                             %
%              Fax: (530) 752-4158                                   %
%              Email: hhcheng@ucdavis.edu                            %
%              WWW:   http://iel.ucdavis.edu/people/cheng.html       %
%              May 7, 1994                                           %
% Modified: February 16, 2001 by Harry H. Cheng                      %
% Modified: January  01, 2003 by Geoffrey R. Shiflett                %
% Use at your own risk, send complaints to /dev/null                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Hoch 3 beispielbilder
% hoch 1, schräg 2 weitere



%%% use twocolumn and 10pt options with the asme2ej format
\documentclass[twocolumn,10pt]{asme2ej}

\usepackage{graphicx} %% for loading jpg figures
\usepackage{hyperref}   % to set up hyperlinks
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue,
	urlcolor=blue,
}
\usepackage[square,numbers]{natbib}
\usepackage{float}
\usepackage{tikz}
\usepackage{amsmath}


\newcommand*\redcircled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt, fill=red!40] (char) {#1};}}
\newcommand*\bluecircled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt, fill=blue!40] (char) {#1};}}


% \usepackage{url}
% \usepackage{breakurl}
% \usepackage[breaklinks]{hyperref}   
%\def\UrlBreaks{\do\/\do-}

%% The class has several options
%  onecolumn/twocolumn - format for one or two columns per page
%  10pt/11pt/12pt - use 10, 11, or 12 point font
%  oneside/twoside - format for oneside/twosided printing
%  final/draft - format for final/draft copy
%  cleanfoot - take out copyright info in footer leave page number
%  cleanhead - take out the conference banner on the title page
%  titlepage/notitlepage - put in titlepage or leave out titlepage
%  
%% The default is oneside, onecolumn, 10pt, final


\title{Computer Vision SS22 Assignment 2: Camera Calibration}

%%% first author
\author{Felix Hamburger
    \affiliation{
	Student ID: 35925\\
	Computer Vision SS22\\
	Computer Science Master\\
	Ravensburg Weingarten University\\
    Email: felix.hamburger@rwu.de
    }	
}

%%% second author
%%% remove the following entry for single author papers
%%% add more entries for additional authors
\author{Mario Amann
    \affiliation{ 
    Student ID: 35926\\
    Computer Vision SS22\\
    Computer Science Master\\
    Ravensburg Weingarten University\\
    Email: mario.amann@rwu.de
     }	
}



\begin{document}

\maketitle   

% Notes:
% Citations
% https://answers.opencv.org/question/32411/opencv-bibtex-citation/
% http://citebay.com/how-to-cite/opencv/

% Functions:

% imread: https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56

% cvtColor: https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html
% RGB[A] to Gray:Y←0.299⋅R+0.587⋅G+0.114⋅B


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
{\it This report presents a usecase for a camera calibration. 
The explanations contained in this report will be divided into @@ steps.
First a preprocessing step creates images from a video, then reads the created images and performs basic operations on the image.
Then corner points of the document will be detected.

By performing a perspective transformation, the document will then be provided in a top-view.
With the help of a filter the document will then be changed into a binary format.
}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{nomenclature}
% \entry{A}{You may include nomenclature here.}
%\entry{$\alpha$}{There are two arguments for each entry of the nomemclature environment, the symbol and the definition.}
% \end{nomenclature}

%The primary text heading is  boldface and flushed left with the left margin.  The spacing between the  text and the heading is two line spaces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\noindent
In everyday life, there are often situations in which it is advantageous to scan a document that has
been received and keep it as a copy. Unfortunately not everyone has a ready-to-use scanner with them to scan the document.
Though most of the people have a device capable of doing just this - a cell phone with a camera.
Using various methods, it is possible to scan documents with the help of the cell phone camera, similar to a scanner, 
and save them in a suitable format.
This report will give a step by step guide on how it is possible to implement a document scanner with Python and OpenCV\cite{2014opencv}.

% This article illustrates preparation of ASME paper using \LaTeX2\raisebox{-.3ex}{$\epsilon$}. The \LaTeX\  macro \verb+asme2ej.cls+, the {\sc Bib}\TeX\ style file \verb+asmems4.bst+, and the template \verb+asme2ej.tex+ that create this article are available on the WWW  at the URL address \url{http://iel.ucdavis.edu/code/}. To ensure compliance with the 2003 ASME MS4 style guidelines  \cite{asmemanual}, you should modify neither the \LaTeX\ macro \verb+asme2ej.cls+ nor the {\sc Bib}\TeX\ style file \verb+asmems4.bst+. By comparing the output generated by typesetting this file and the \LaTeX2\raisebox{-.3ex}{$\epsilon$} source file, you should find everything you need to help you through the preparation of ASME paper using \LaTeX2\raisebox{-.3ex}{$\epsilon$}. Details on using \LaTeX\ can be found in \cite{latex}. 

% In order to get started in generating a two-column version of your paper, please format the document with 0.75in top margin, 1.5in bottom margin and 0.825in left and right margins.  Break the text into two sections one for the title heading, and another for the body of the paper.  

% The format of the heading is not critical, on the other hand formatting of the body of the text is the primary goal of this exercise.  This will allow you to see that the figures are matched to the column width and font size of the paper.  The double column of the heading section is set to 1.85in for the first column, a 0.5in spacing, and 4.5in for the second column.  For the body of the paper, set it to 3.34in for both columns with 0.17in spacing, both are right justified. 

% The information that is the focus of this exercise is found in 
% section~\ref{sect_figure}.
% Please use this template to format your paper in a way that is similar to the printed form of the Journal of Mechanical Design.  This will allow you to verify that the size and resolution of your figures match the page layout of the journal.  The ASME Journal of Mechanical Design will no longer publish papers that have the errors demonstrated here.

% ASME simply requires that the font should be the appropriate size and not be blurred or pixilated, and that lines should be the appropriate weight and have minimal, preferably no, pixilation or rasterization.

% The journal uses 10pt Times Roman Bold for headings, but Times Bold is good enough for this effort.  The text is set at 9pt Times Roman, and again Times will be fine.  Insert a new line after the heading, and two lines after each section.  This is not exactly right but it is close enough.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preprocessing}
\label{section:preprocessing}
\noindent
Before any preprocessing can be applied, the images have to be created from the given video. Therefore each frame of the video is saved separately. This is done with the imread function\cite{opencv_imread} of OpenCV which 
outputs the image as numpy array with the shape (Height, Width, Channel). By default the decoded image is stored in the BGR-format. Which means, the first channel contains the
blue color channel, the second one contains the green color channel and the last the red one. In preparation for the detection of the checkerboard pattern, the images are converted to grayscale format. 
This convertion is made with the cvtColor function\cite{opencv_cvtColor} of OpenCV. The convertion of colorspace is made with the following
formula\cite{opencv_rgb2gray}:
\begin{center}
    $Y = B * 0.114 + G * 0.587 + R * 0.299$
    \label{eq_rgb2gray}
\end{center}



%findChessboardCorners: https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Corner Detection}
\label{section:edgedetection}
% edges = Canny(blurred_image,75,180, L2gradient=True)
\noindent
After the preprocessing steps, the corners of the chessboard can be detected on the images. For this the OpenCV function findChessboardCorners\cite{opencv_findChessboardCorners} is used. As input parameters the function gets a grayscale image, the size of the pattern and already recognized corners. As size of the pattern the size 6 times 6 was chosen, because in about 500 pictures from 2000 the chessboard is recognized and the chessboard is recognized at completely different places. Since no corners are recognized before, nothing is passed at the already recognized corners. Furthermore, no flag is used and needed because the images were already converted to grayscale in preprocessing, incorrectly extracted squares were manually checked and did not occur, and there was no time pressure to speed up the detection of the corners.
If corners of a chessboard are found in the image, the corner points are added to a list of corner points from all images.


% After preprocessing steps are taken and a suitable noise reduction algorithm was chosen, the edge detection can be started.
% For edge detection the Canny edge\cite{canny_paper} detector is used, which is a multi-staged algorithm:
% \begin{enumerate}
%     \item Noise reduction.
%     \item Calculating the intensity gradient of the image.
%     \item Non-maximum supression.
%     \item Hysteresis thresholding.
% \end{enumerate}
% The noise reduction step was already described in the \nameref{section:preprocessing} section of this paper.
% \\\\
% The intensity gradient of the image is calculated using the \textbf{Sobel Filter}:
% \begin{equation}
%     S(I(x,y)) :=\sqrt{(S_x*I(x,y))^2 + S_y(*I(x,y))^2}
% \end{equation}
% Generally we can define the gradient of the Image I as:
% \begin{equation}
%     G(I(x,y)) := \sqrt[]{I_x^2 +I_y^2}
% \end{equation}
% \noindent
% In addition to the gradient we calculate the the orientation given by:
% \begin{equation}
%     \phi(x,y) = arctan(\frac{g_y}{g_x})
% \end{equation}

% \noindent
% After getting the gradient magnitude and orientation of each pixel, the edges have
% to be reduced to a thickness of one pixel which will be realized with \textbf{non-maximum supression.}
% \\
% Lastly it is decided whether a pixel will be mapped to 1 or 0 with application of \textbf{hysteresis.}\\\\
% \noindent
% To implement this method in the document scanner, the function
% which implements the full Canny edge detector\cite{opencv_canny} can be called:
% \begin{center}
%     \noindent
%     $Canny(blurred\_image,75, 180,$\\
%     $L2gradient = True, apertureSize = 3)$
% \end{center}
% \noindent
% The functions input is the noise reduced image described in \nameref{section:preprocessing}.
% For Hysteresis Threshold 75 for the lower-bound and 180 for the upper bound are chosen.
% In several trials this settings could sufficiently provide the overall best results 
% when setting the aperture size to 3.
% Instead of the predefined L1-Norm \cite{l1_norm}, the more precise L2-Norm\cite{l2_norm} is used.
% \\\\
% The Canny edge detection filter returns an binary output image with edges
% being set to 1 and other points being set to 0.
% \\\\
% This can be seen in Figure \ref{fig:canny}

% \begin{figure}[H]
% \centerline{\includegraphics[width=2.5in]{output/hoch_3_4_canny.jpg}}
% \caption{The output image of the Canny edge detection.}
% \label{fig:canny}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Camera Calibration}
\noindent

With the list of corner points the intrinsic camera parameters can be estimated. 
The OpenCV function calibrateCamera\cite{opencv_calibrateCamera} is used, which is based on the algorithm of the paper by Z.Zhang\cite{zhang2000flexible} from 2000. 
The first parameter passed is a matrix consisting of vectors of the calibration points in the coordinate space. In this case this matrix is a multi-dimensional meshgrid. Furthermore the list of corner points from the \nameref{section:edgedetection} section and the size of the image are passed to the function. The remaining passing parameters are passed as NoneType and no flags are set. The function returns the root mean squared re-projection error, the intrinsic camera matrix, the distortion coefficients and the rotation and translation vectors.
The intrinsic camera matrix is described as follows: 
\begin{center}
    $K = \begin{bmatrix}
   f_x & 0 & c_x \\
   0 & f_y & c_y \\
   0 & 0 & 1
   \end{bmatrix}
   $
    \label{eq_intrinsicmatrix}
\end{center}
The returned intrinsic camera matrix from the calibrateCamera function has the values:
@@@@@@@@@@@@@@@@@@@@
\begin{center}
    $K = \begin{bmatrix}
   f_x & 0 & c_x \\
   0 & f_y & c_y \\
   0 & 0 & 1
   \end{bmatrix}
   $
    \label{eq_calculatedintrinsicmatrix}
\end{center}
The calibrateCamera function returns following distortion coefficients:
@@@@@@@@@@@@@@@@@@@@
\begin{center}
    $k_1 = s$\\
    $k_2 = s$\\
    $p_1 = s$\\
    $p_2 = s$\\
    $k_1 = s$\\
    $k_1 = s$
    \label{eq_distcoeff}
\end{center}
As next step the optimal camera intrinsic matrix has to be computed. For this purpose the getOptimalNewCameraMatrix function\cite{opencv_getOptimalNewCameraMatrix} is used. The function computes the optimal camera intrinsic matrix based on free scaling parameter alpha.
The camera matrix, the distortion coefficients vector, the original and the new image size and the free scaling parameter alpha are passed as input. No further optional parameter are passed. The camera matrix and the distortion coefficients vector are the previously calculated values. As original and new image size the original size of the image is passed. The free scaling parameter alpha is set to one. Therefore all pixels are kept and no pixels are removed \cite{opencv_calibrationtutorial}.
The optimal intrinsic camera matrix from getOptimalNewCameraMatrix has the following values:
@@@@@@@@@@@@@@@@@@@@
\begin{center}
    $K = \begin{bmatrix}
   f_x & 0 & c_x \\
   0 & f_y & c_y \\
   0 & 0 & 1
   \end{bmatrix}
   $
    \label{eq_optintrinsicmatrix}
\end{center}
With the optimal intrinsic matrix the next step the undistortion can take place.









% To find the corner of the document the previously generated edges 
% in \nameref{section:edgedetection} have to be evaluated.
% For this purpose, contours are formed which are represented by a curve that connects all 
% continuous points with each other\cite{SUZUKI198532}.\\\\
% \noindent
% The OpenCV function\cite{opencv_findcontours} is used as follows:
% \begin{center}
%     \noindent
%     $findContours(edges,$\\
%     $RETR\_LIST, CHAIN\_APPROX\_SIMPLE)$
% \end{center}
% \noindent
% Where the input are the previously generated edges, the retrieval 
% mode\cite{opencv_retrievalmode} which is 
% set to retrieve the contours without establishing any hierarchical relationships 
% and the contour approximation mode\cite{opencv_approxmode} which is set to output only their endpoints.
% (E.g: A rectangular contour is defined by the four corner points)
% This can be seen in Figure \ref{fig:contours}\\\\
% \noindent
% The assumption is made that the document is the largest polygon in the image.
% The contours are hierarchically sorted according to their area, with the largest polygon with four vertices being selected\cite{RAMER1972244}\cite{doi:10.3138/FM57-6770-U75U-7727}.
% The polygon is found by using following functions\cite{opencv_approxpoly}\cite{opencv_arclength}:

% \begin{center}
%     \noindent
%     $arcLength(contour,true)$
% \end{center}

% \begin{center}
%     \noindent
%     $approxPolyDP(contour,$\\
%     $epsilon, true)$
% \end{center}
% \noindent
% Where we first calculate the curve perimeter which will be used as 
% input for the approximation. 
% The approximation accuracy is set to $0.05$ times the curve perimeter and 
% the bool is set to true, which indicates that the curve must be closed, thus the first and last vertices are connected.


% \begin{figure}[H]
% \centerline{\includegraphics[width=2.5in]{output/hoch_3_5_contouredimage.jpg}}
% \caption{The corners of the document.}
% \label{fig:contours}
% \end{figure}





% findContours
% Paper:
% https://reader.elsevier.com/reader/sd/pii/0734189X85900167?token=11C96FBB310332A0D41181ED5B29CA355D79630C93940FA6DCEABD7D12A580B1F46464823FDA2CB7D695CA063C30B005&originRegion=eu-west-1&originCreation=20220509094855
% contours = sorted(contours, key=contourArea, reverse=True)
%epsilon = 0.1*arcLength(contour,True)
%approx = approxPolyDP(contour,epsilon,True)


\section{Undistortion}
\label{section:transformation}
\noindent


\section{Summary}


% % Here's where you specify the bibliography style file.
% % The full file name for the bibliography style file 
% % used for an ASME paper is asmems4.bst.
% \bibliographystyle{asmems4}
\bibliographystyle{asmems4}

% Here's where you specify the bibliography database file.
% The full file name of the bibliography database for this
% article is asme2e.bib. The name for your database is up
% to you.
\bibliography{asme2e}


\end{document}
